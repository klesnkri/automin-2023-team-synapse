{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import itertools\n",
    "\n",
    "\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "\n",
    "OUTPUT_DIR = \"preprocessed_data/elitr\"\n",
    "\n",
    "ELITR_EN_DIR = \"../../datasets/ELITR Minuting Corpus/ELITR-minuting-corpus/elitr-minuting-corpus-en\"\n",
    "ELITR_AUTOMIN_2023_DIR = \"../../datasets/automin-2023-data/Task-A\"\n",
    "\n",
    "TRAIN_DIR = \"train\"\n",
    "DEV_DIR = \"dev\"\n",
    "TEST_DIR = \"test\"\n",
    "TEST2_DIR = \"test2\"\n",
    "AUTOMIN_EN_DIR = \"test2023-en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def read_transcripts(meetings_dir):\n",
    "    transcripts = {}\n",
    "\n",
    "    for meeting_id in sorted(os.listdir(meetings_dir)):\n",
    "        meeting_dir = os.path.join(meetings_dir, meeting_id)\n",
    "        transcript_file = glob.glob(os.path.join(meeting_dir, \"transcript_*.txt\"))[0]\n",
    "\n",
    "        with open(transcript_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            transcript = f.read().splitlines()\n",
    "            transcripts[meeting_id] = transcript\n",
    "\n",
    "    return transcripts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "en_train = read_transcripts(os.path.join(ELITR_EN_DIR, TRAIN_DIR))\n",
    "en_dev = read_transcripts(os.path.join(ELITR_EN_DIR, DEV_DIR))\n",
    "en_test = read_transcripts(os.path.join(ELITR_EN_DIR, TEST_DIR))\n",
    "en_test2 = read_transcripts(os.path.join(ELITR_EN_DIR, TEST2_DIR))\n",
    "en_automin2023 = read_transcripts(os.path.join(ELITR_AUTOMIN_2023_DIR, AUTOMIN_EN_DIR))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def parse_transcript_by_speaker(transcript):\n",
    "    parsed_transcipt = []\n",
    "\n",
    "    for line in transcript:\n",
    "        if line.startswith(\"(PERSON\"):\n",
    "            match = re.match(r\"\\((PERSON\\d?\\d?)\\)(.*)\", line)\n",
    "            role, utterance = match.group(1), match.group(2).strip()\n",
    "            parsed_transcipt.append({\"role\": role, \"utterance\": [utterance]})\n",
    "        elif len(parsed_transcipt) > 0:\n",
    "            parsed_transcipt[-1][\"utterance\"].append(line.strip())\n",
    "\n",
    "    return parsed_transcipt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def remove_asr_errors(tokens):\n",
    "    ASR_STOPWORDS = [r\"u+h+m*-?\", r\"m*h+m+-?\", r\"u+m+-?\", r\"e+h+m*-?\", r\"e*m+-?\", r\"e+r+m+-?\", r\"a+h+\", r\"u+h+n+-?\", r\"h+u+(h|m)+-?\"]\n",
    "    ASR_STOPWORDS_COMBINATIONS = [f\"{stop0}-{stop1}\" for stop0, stop1 in (itertools.combinations(ASR_STOPWORDS, 2))]\n",
    "\n",
    "    # Remove ASR stopwords\n",
    "    filtered_tokens = [token for token in tokens if not any(re.fullmatch(regex, token.lower()) for regex in ASR_STOPWORDS + ASR_STOPWORDS_COMBINATIONS)]\n",
    "\n",
    "    # Words ending with '-' -> remove if prefix of next word or just remove '-' from end\n",
    "    filtered_tokens2 = []\n",
    "\n",
    "    for idx, token in enumerate(filtered_tokens):\n",
    "        if token == \"-\" or not token.endswith(\"-\"):\n",
    "            filtered_tokens2.append(token)\n",
    "        elif idx == len(filtered_tokens)-1 or not filtered_tokens[idx+1].lower().startswith(token[:-1].lower()):\n",
    "            filtered_tokens2.append(token[:-1])\n",
    "\n",
    "    return filtered_tokens2\n",
    "\n",
    "def remove_tags(text):\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = re.sub(r\"\\(?\\)\", \"\", text)\n",
    "    text = re.sub(r\"\\[\", \"\", text)\n",
    "    text = re.sub(r\"]\", \"\", text)\n",
    "    text = re.sub(r\"\\(\", \"\", text)\n",
    "    text = re.sub(r\"\\)\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def is_punct(str):\n",
    "    return all(c in string.punctuation + \"â€“\" for c in str)\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Remove tags\n",
    "    text = remove_tags(text)\n",
    "\n",
    "    # Tokenizer and detokenizer\n",
    "    tokenizer = MosesTokenizer(lang=\"en\")\n",
    "    detokenizer = MosesDetokenizer(lang=\"en\")\n",
    "\n",
    "    # Tokenize and remove ASR errors\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = remove_asr_errors(tokens)\n",
    "\n",
    "    # Remove punctuation at the start of sentence\n",
    "    try:\n",
    "        first_non_punct_idx = next(idx for idx, token in enumerate(tokens) if not is_punct(token))\n",
    "        tokens = tokens[first_non_punct_idx:]\n",
    "    except StopIteration:\n",
    "        tokens = []\n",
    "\n",
    "    if len(tokens) > 0:\n",
    "        # Remove consecutive duplicates\n",
    "        tokens = [token for idx, token in enumerate(tokens) if idx == 0 or token.lower() != tokens[idx-1].lower()]\n",
    "\n",
    "        # Remove consecutive punctuation\n",
    "        tokens = [token for idx, token in enumerate(tokens) if idx == len(tokens) - 1 or not is_punct(tokens[idx]) or not is_punct(tokens[idx+1])]\n",
    "\n",
    "        # Start sentence with uppercase\n",
    "        tokens[0] = tokens[0][0].upper() + tokens[0][1:]\n",
    "\n",
    "        # End sentence with punctuation\n",
    "        if not is_punct(tokens[-1][-1]):\n",
    "            tokens.append(\".\")\n",
    "\n",
    "    # Detokenize\n",
    "    return detokenizer.detokenize(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def preprocess_transcript(transcript):\n",
    "    roles = []\n",
    "    utterances = []\n",
    "\n",
    "    for line in transcript:\n",
    "        normalized_utterance = [normalize_text(sentence) for sentence in line[\"utterance\"]]\n",
    "        normalized_utterance = \" \".join(sentence for sentence in normalized_utterance if len(sentence) > 0)\n",
    "\n",
    "        if len(normalized_utterance) > 0:\n",
    "            roles.append(line[\"role\"])\n",
    "            utterances.append(normalized_utterance)\n",
    "\n",
    "    assert len(roles) == len(utterances)\n",
    "    return {\"roles\": roles, \"utterances\": utterances}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def preprocess_transcripts(transcripts):\n",
    "    preprocessed_transcripts = {}\n",
    "\n",
    "    for meeting_id, transcript in transcripts.items():\n",
    "        preprocessed_transcript = parse_transcript_by_speaker(transcript)\n",
    "        preprocessed_transcripts[meeting_id] = preprocess_transcript(preprocessed_transcript)\n",
    "\n",
    "    return preprocessed_transcripts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "en_train_preprocessed = preprocess_transcripts(en_train)\n",
    "en_dev_preprocessed = preprocess_transcripts(en_dev)\n",
    "en_test_preprocessed = preprocess_transcripts(en_test)\n",
    "en_test2_preprocessed = preprocess_transcripts(en_test2)\n",
    "en_automin2023_preprocessed = preprocess_transcripts(en_automin2023)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def save_preprocessed(preprocessed, output_dir, output_file):\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, output_dir), exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(OUTPUT_DIR, output_dir, f\"{output_file}.json\"), \"w\") as f:\n",
    "        json.dump(preprocessed, f, ensure_ascii=False, indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "save_preprocessed(en_train_preprocessed, \"en\", TRAIN_DIR)\n",
    "save_preprocessed(en_dev_preprocessed, \"en\", DEV_DIR)\n",
    "save_preprocessed(en_test_preprocessed, \"en\", TEST_DIR)\n",
    "save_preprocessed(en_test2_preprocessed, \"en\", TEST2_DIR)\n",
    "save_preprocessed(en_automin2023_preprocessed, \"en\", AUTOMIN_EN_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}